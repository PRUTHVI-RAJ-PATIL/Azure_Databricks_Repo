{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd065dcc-1ff7-4c12-b622-5dcc3b4a8deb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "# ignore , here im generating the more sample data \n",
    "\n",
    "import random\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark Session\n",
    "\n",
    "\n",
    "# Existing Sample Data\n",
    "simpleData = [\n",
    "    (\"James\", \"Sales\", \"NY\", 90000, 34, 10000),\n",
    "    (\"Michael\", \"Sales\", \"NY\", 86000, 56, 20000),\n",
    "    (\"Robert\", \"Sales\", \"CA\", 81000, 30, 23000),\n",
    "    (\"Maria\", \"Finance\", \"CA\", 90000, 24, 23000),\n",
    "    (\"Raman\", \"Finance\", \"CA\", 99000, 40, 24000),\n",
    "    (\"Scott\", \"Finance\", \"NY\", 83000, 36, 19000),\n",
    "    (\"Jen\", \"Finance\", \"NY\", 79000, 53, 15000),\n",
    "    (\"Jeff\", \"Marketing\", \"CA\", 80000, 25, 18000),\n",
    "    (\"Kumar\", \"Marketing\", \"NY\", 91000, 50, 21000)\n",
    "]\n",
    "\n",
    "# Generate 100 Random Records\n",
    "departments = [\"Sales\", \"Finance\", \"Marketing\", \"IT\", \"HR\"]\n",
    "states = [\"NY\", \"CA\", \"TX\", \"FL\", \"IL\"]\n",
    "\n",
    "for i in range(100):\n",
    "    name = f\"Employee_{i+1}\"\n",
    "    department = random.choice(departments)\n",
    "    state = random.choice(states)\n",
    "    salary = random.randint(50000, 120000)  # Salary between 50,000 and 120,000\n",
    "    age = random.randint(22, 60)           # Age between 22 and 60\n",
    "    bonus = random.randint(5000, 30000)   # Bonus between 5,000 and 30,000\n",
    "    simpleData.append((name, department, state, salary, age, bonus))\n",
    "\n",
    "# Define Schema\n",
    "schema = [\"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data=simpleData, schema=schema)\n",
    "\n",
    "# Show the first 20 rows of the expanded DataFrame\n",
    "print(\"Expanded DataFrame with 109 Records:\")\n",
    "df.show(20)\n",
    "\n",
    "# Count total records to verify\n",
    "print(f\"Total Records: {df.count()}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "testing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
